{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical no. 7\n",
    "# finite state automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Define grammar using nltk. Analyze a sentence using the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'that', 'flight']\n",
      "(S (VP (VP Book) (NP (Det that) (NP flight))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> VP\n",
    " VP -> VP NP\n",
    " NP -> Det NP\n",
    " Det -> 'that'\n",
    " NP -> singular Noun\n",
    " NP -> 'flight'\n",
    " VP -> 'Book' \n",
    "\"\"\")\n",
    "sentence = \"Book that flight\"\n",
    "for index in range(len(sentence)):\n",
    "    all_tokens = tokenize.word_tokenize(sentence)\n",
    "print(all_tokens)\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.  Accept the input string with Regular expression of Finite Automaton: 101+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FA(s):\n",
    "    #if the length is less than 3 then it can't be accepted, Therefore end the process.\n",
    "    if len(s)<3:\n",
    "        return \"Rejected\"\n",
    "    #first three characters are fixed. Therefore, checking them using index\n",
    "    if s[0]=='1':\n",
    "        if s[1]=='0':\n",
    "            if s[2]=='1':\n",
    "                # After index 2 only \"1\" can appear. Therefore break the process if any othercharacter is detected\n",
    "                for i in range(3, len(s)):\n",
    "                    if s[i]!='1':\n",
    "                        return \"Rejected\"\n",
    "                    return \"Accepted\"\n",
    "                return \"Rejected\"\n",
    "            return \"Rejected\"\n",
    "        return \"Rejected\"\n",
    "    inputs=['1','10101','101','10111','01010','100','','10111101','1011111']\n",
    "    for i in inputs:\n",
    "        print(FA(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Accept the input string with Regular expression of FA: (a+b)*bba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FA(s):\n",
    "    size=0\n",
    "    #scan complete string and make sure that it contains only 'a' & 'b'\n",
    "    for i in s:\n",
    "        if i=='a' or i=='b':\n",
    "            size+=1\n",
    "        else:\n",
    "            return \"Rejected\"\n",
    "    #After checking that it contains only 'a' & 'b'\n",
    "    #check it's length it should be 3 atleast\n",
    "    if size>=3:\n",
    "        #check the last 3 elements\n",
    "        if s[size-3]=='b':\n",
    "            if s[size-2]=='b':\n",
    "                if s[size-1]=='a':\n",
    "                    return \"Accepted\"\n",
    "                return \"Rejected\"\n",
    "            return \"Rejected\"\n",
    "        return \"Rejected\"\n",
    "    return \"Rejected\"\n",
    "inputs=['bba', 'ababbba', 'abba','abb', 'baba','bbb','']\n",
    "for i in inputs:\n",
    "    print(FA(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Implementation of Deductive Chart Parsing using context free grammar and a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (Det a) (N bird)))\n",
      "    (PP (P in) (NP (Det my) (N balcony)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N bird) (PP (P in) (NP (Det my) (N balcony))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'a' | 'my'\n",
    "N -> 'bird' | 'balcony'\n",
    "V -> 'saw'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "sentence = \"I saw a bird in my balcony\"\n",
    "for index in range(len(sentence)):\n",
    "    all_tokens = tokenize.word_tokenize(sentence)\n",
    "print(all_tokens)\n",
    "# all_tokens = ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
