{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Practical 11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrcMiOHkSBGHL9kuPeQc9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyansupadhyay7/NLP-Practicals/blob/main/Practical%20No.%2011/NLP_Practical_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical no. 11\n",
        "# A. Multi word expressions in NLP"
      ],
      "metadata": {
        "id": "2p6iQ1IJQ7Er"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69k39B1QQcMZ",
        "outputId": "405cf7ed-d927-4d98-df55-267532baf382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n",
        "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n",
        "for sent in sent_tokenize(s):\n",
        " print(mwe.tokenize(word_tokenize(sent)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Normalized web distance and word similarity"
      ],
      "metadata": {
        "id": "EVhJP02gRhuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re \n",
        "!pip install textdistance\n",
        "import textdistance\n",
        "# we will need scikit-learn>=0.21\n",
        "import sklearn #pip install sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts = ['Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market','Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport', 'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n",
        "def normalize(text):\n",
        " \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        " return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
        "def group_texts(texts, threshold=0.4): \n",
        " \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        " normalized_texts = np.array([normalize(text) for text in texts])\n",
        " distances = 1 - np.array([\n",
        " [textdistance.jaro_winkler(one, another) for one in normalized_texts] \n",
        " for another in normalized_texts\n",
        " ])\n",
        " clustering = AgglomerativeClustering(\n",
        " distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
        " affinity=\"precomputed\", linkage=\"complete\", n_clusters=None\n",
        " ).fit(distances)\n",
        " centers = dict()\n",
        " for cluster_id in set(clustering.labels_):\n",
        "  index = clustering.labels_ == cluster_id\n",
        "  centrality = distances[:, index][index].sum(axis=1)\n",
        "  centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        " return [centers[i] for i in clustering.labels_]\n",
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBfIOS2oRK4P",
        "outputId": "966b7c26-b1b8-49c9-937d-1ca3744415e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.2\n",
            "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Word Sense Disambiguation"
      ],
      "metadata": {
        "id": "l8tCkLZbR1lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Sense Disambiguation\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_first_sense(word, pos=None):\n",
        " if pos:\n",
        "  synsets = wn.synsets(word,pos)\n",
        " else:\n",
        "  synsets = wn.synsets(word)\n",
        " return synsets[0]\n",
        "best_synset = get_first_sense('bank')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','n')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','v')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdlx2KeiRuhz",
        "outputId": "80b8304c-2ca4-48b1-d0af-0bbc39211472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "omva88dASCMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}