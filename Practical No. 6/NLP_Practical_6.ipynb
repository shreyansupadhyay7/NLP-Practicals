{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Practical 6",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1tVkGnev2z1o1V0QHTPYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyansupadhyay7/NLP-Practicals/blob/main/Practical%20No.%206/NLP_Practical_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6a Parts of speech tagging and chunking of user defined text"
      ],
      "metadata": {
        "id": "vJtT81hIh0GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "from nltk import chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53uu97ehO4m",
        "outputId": "1e5dcffd-4642-4b5b-d431-f06d60314d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello! this is Shreyans. Today we will be learning NLP\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\",sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em0jScBwieo-",
        "outputId": "84fb52de-6dcf-45bd-faac-f3b5e7899ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Hello!', 'this is Shreyans.', 'Today we will be learning NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "print(\"\\nword tokenization\\n======\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI2LZzjzi5Kv",
        "outputId": "29ded108-7048-4436-e5cc-c3c48009bebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word tokenization\n",
            "======\n",
            "\n",
            "['Hello', '!']\n",
            "['this', 'is', 'Shreyans', '.']\n",
            "['Today', 'we', 'will', 'be', 'learning', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS TAGGING\n",
        "tagged_words=[]\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "  print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDBC6hlLjXPt",
        "outputId": "0a983efb-2f02-4f45-c272-b6f2fe2d4880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunki8ng\n",
        "tree = []\n",
        "for index in range(len(sents)):\n",
        "  tree.append(chunk.ne_chunk(tagged_words[index]))\n",
        "print(\"\\nchunking\\n===========\")\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkIC6h0aj2sW",
        "outputId": "ec12466b-fbe6-4d11-821e-996fdd485763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "chunking\n",
            "===========\n",
            "[Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])]), Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])]), Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#b. Named Entity recognition using user defined text."
      ],
      "metadata": {
        "id": "44w2AEkumE32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "hkepXc3Ql0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "nm8FVh7lmNrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Process whole documnet\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "\"Google in 2007, few people outside of the company took him \"\n",
        "\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "\"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "\"this week.\")\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "tbnrFjIEmVec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyze syntax\n",
        "print(\"Noun phrase: \", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verb\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkT-CWU3mmef",
        "outputId": "ad6b8cc6-3024-470f-ba7e-7f97b281be7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrase:  ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verb ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Verb\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqfjP0HunRA2",
        "outputId": "710ad6aa-e088-411a-8661-0325d6dacfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verb ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    }
  ]
}